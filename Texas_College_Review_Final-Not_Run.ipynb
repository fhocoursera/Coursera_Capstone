{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texas College Review\n",
    "Author: Mr. Frederick Orndorff\n",
    "Date: January 26, 2020\n",
    "\n",
    "## Code Documentation:\n",
    "Most of the code below is from the Coursera labs within the __IBM Data Science__ Specialization.  I have modified the code to support my business problem.\n",
    "\n",
    "## Introduction\n",
    "__Business Problem:__\n",
    "There are over 5,300 Colleges and Universities in the United States and over 450 of these colleges are in the state of Texas.  There are so many colleges in the Texas that a person could spend months (or even years) attempting to determine the best fit for a student -- or to determine which college is the best in Texas.  There are many personal opinions on which college ranks first, but can we use data to determine the best college in Texas?  \n",
    "\n",
    "For this project I will use public web data and the Foursquare API to provide a ranking for some Texas colleges.  This will be a data driven solution ot the business problem of too many colleges to determine the best fit.\n",
    "\n",
    "__Stakeholders:__\n",
    "There are many stakeholders in this problem, including: students, parents (who might pay for the student to attend college), transfer students, as well as, the college administration and presidents.\n",
    "\n",
    "## Data\n",
    "__Map Data:__\n",
    "I plan to create a choropeth map to provide a visual representation of crime and other data.  In order to adequately display the map I wanted to overlay the Texas counties on the state map.  The Texas state government provided the data used for the county overlays, located [at this link.](https://data.texas.gov/dataset/Texas-Counties-Centroid_Map/ups3-9e8m).  This dataset also provided the latitude and longitude coordinates needed to use the Foursquare API data.\n",
    "\n",
    "__Informational Datasets:___\n",
    "In addition to the county information I also found the following datasets that I will incooporate in our data driven solutions:\n",
    "\n",
    "1. [2018 Texas Crime Data](http://www.dps.texas.gov/administration/crime_records/pages/crimestatistics.htm)\n",
    "\n",
    "The Texas Department of Public safety compiles data from state and local law enforcement agencies across the country, as well as the FBI. The Interstate Identification Index is a cooperative program between the FBI and the states to facilitate the exchange of information among those states and the FBI in order to reduce duplication of effort in records retention and demands accurate information for success. Crime data is focused on violent crimes, hate crimes, and assaults.\n",
    "\n",
    "2. [Texas State Expenditures by County](https://data.texas.gov/Government-and-Taxes/Texas-State-Expenditures-by-County/f2iw-dtqt)\n",
    "\n",
    "Texas State Expenditures by County shows where state dollars are spent. It lists state expenditures by agency and type of expenditure in each county. This data provides an overview of where the State is spending the local tax dollars for improvement(s).\n",
    "\n",
    "3. [Mixed Beverage Sales](https://data.texas.gov/Government-and-Taxes/Mixed-Beverage-Gross-Receipts/naix-2893)\n",
    "\n",
    "This file contains a list of taxpayers required to report mixed beverage gross receipts tax reports under Texas law.  It provides total sales reported and taxes paid.\n",
    "\n",
    "4. [List of Texas Colleges](http://www.txhighereddata.org/Interactive/Institutions.cfm)\n",
    "\n",
    "This database provides the list of Colleges, Universities, and trade schools available in the state of Texas.\n",
    "\n",
    "## Data Application to the Business Problem\n",
    "\n",
    "__Assumptions___\n",
    "I will make the following assumptions for the stakeholders:\n",
    "1. Lower crime is better\n",
    "2. Higher state expenditures in the county is better\n",
    "3. Larger beverage sales is better (work hard - play harder!!)\n",
    "4. Larger number of Foursquare data (venues) in the area is better.\n",
    "\n",
    "## Methodology  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python packages used for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, geocoder, requests, json, folium, geopy\n",
    "import matplotlib.cm as cm, matplotlib.colors as colors, matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used some older python packages for this project since I was familiar with the correct syntax of the packages.  However, some of the packages resulted in 'warnings' from Python -- to use the updated package.  The below code will temporarily turnoff these warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set working directoy\n",
    "#os.chdir(\"/Users/christinaorndorff/Documents/Fred's Stuff/Coursa/IBM_Capstone\") #for MAC\n",
    "os.chdir(\"C:/Users/OrndorFH01/Coursera/IBM_Capstone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "__Texas County Map Data:__\n",
    "There are 254 counties in Texas, therefore, the Texas county data has 254 rows of data.  Each row contains the latitude and longitude coordinates, the county name, the county number, the Federal Information Processing Standards code (provided by the U.S. census), the size of the county (including the land area), and the county location (which is the lat/long coordinates together)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Texas_Counties_Centroid_Map.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was only interested in four columns: latitude, longitude, county name, and county number.  All other columns were removed during data cleaning.  I kept the county number since another data base only contains the county number vice the county name, therefore, I need to keep the county number to adequately merge the two dataframes (this occurs in a later step).  Additionally, we had to make the county name lowercase to match the 'case' in other databases.\n",
    "\n",
    "One major issue I found in the database was the incorrect labeling of the latitude and longitude coordinates.  For some reason the two coordinates were backwards (swapped).  I initially did not see this error, until I attempted to visualize the data on a folium map and it did not render correcly.  After many attempts and some in depth analysis - I determined the coordinates were backwards.  Once corrected - everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.iloc[:, [0, 1, 2, 3]]\n",
    "#for some reason the base .csv file has the Lat/Long values swapped -- we can fix this by renaming the columns\n",
    "df1.columns = ['Longitude', 'Latitude', 'County_Name', 'County_Number']\n",
    "#we will need to make the 'County_Name' lowercase -- to merge dfs later\n",
    "df1['County_Name'] = df1['County_Name'].str.lower()\n",
    "df1 = df1[['County_Name', 'County_Number', 'Latitude', 'Longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape #254 counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mixed Beverage Sales:__\n",
    "The mixed beverage sales includes over two-million rows and 24 columns of data.  This is a larger file - 453MB.  \n",
    "\n",
    "Some interesting data points include: Location County, Liquor Receipts, Wine Receipts, Beer Receipts, and Total Receipts.  Focusing on these five columns I determined the most important data will be the location county and the total receipts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Mixed_Beverage_Gross_Receipts.csv')\n",
    "df2 = df2[['Location County', 'Total Receipts']]\n",
    "#rename df2 columns\n",
    "df2.columns = ['County_Number', 'Total_Alcohol_Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two small issues arose during inital data analysis.  The first issue was that each reporting venue within the county had a seperate data entry (therefore the 2 million entries).  This created the need to sum all the venues within each county - resulting in 254 unique 'total receipts', one for each county.  Secondly, the 'Location County' was a numerical entry - not text - therefore I needed to merge this dataframe using the county number.  Fortunately, the Texas County Map Data included the county number, so we can easly merge these two dataframes.  I orginally stripped the county number from the county database, but once I conducted some data analysis on the mixed beverage sales - I added the county number back so I could merege these databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum 'Total_Alcohol_Sales' by 'County' -- using the reset_index() function keeps the output as a pandas df\n",
    "df2_sum = df2.groupby('County_Number')['Total_Alcohol_Sales'].sum().reset_index()\n",
    "df2_sum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First Merged Dataframe.__ \n",
    "To ensure the dataframes are compatible (there is a column to merge against), I went ahead and merged the first two dataframes and checked for consistency.  It works!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df1, df2_sum, on = \"County_Number\")\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Texas State Expenditures by Country.__ \n",
    "The Texas state expenditures by country data includes over 88 thousand rows and 6 columns of data. While it is not as big as the mixed beverage sales database - there are some similarities.  Most noticeable is the multiple row entries for each county.  This issue requires me to combine entries on the county to determine a total amount spent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('Texas_State_Expenditures_by_County.csv')\n",
    "\n",
    "#only keep county and amount columns\n",
    "df3 = df3[['County', 'Amount']]\n",
    "\n",
    "#rename columns\n",
    "df3.columns = ['County_Name', 'Expenditure_Amount']\n",
    "\n",
    "#sum the row amounts by county\n",
    "df3_sum = df3.groupby('County_Name')['Expenditure_Amount'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This database created one new issue.  The county names were provided in all uppercase, which did not match our inital database of counties (Texas_Counties_Centroid_Map).  In order to have like columns to merge against - I had to make all the county names lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to make the 'County_Name' lowercase so we can merge the two dfs\n",
    "df3_sum['County_Name'] = df3_sum['County_Name'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this step was complete, I merged this dataframe with first merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the 'Expenditure_Amount' to the final df\n",
    "df_final = pd.merge(df_final, df3_sum, on = \"County_Name\")\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later in the project I created a map with tags.  Unfortunately, I did not like the county displayed in all lower case, so I went bact to the orginal database and added the county names with the correct capitalization (County_Name2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df[['CNTY_NM', 'CNTY_NBR']]\n",
    "df4.columns = ['County_Name2', 'County_Number']\n",
    "df_final = pd.merge(df_final, df4, on = \"County_Number\")\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[Texas Crime Data:](http://www.dps.texas.gov/administration/crime_records/pages/crimestatistics.htm)__\n",
    "While I was researching the background for this topic - best colleges in Texas - I discovered that a major determining factor for college attendance is the crime rate for the surrounding area.  Therefore, I thought the first step in the project would be to create a choropleth map of Texas.  In order to complete this task, I needed another database with county crime data.  This section shows the steps I took to clean and modify the database to use for the choropleth map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('TX_Crime2.csv')\n",
    "\n",
    "#sum incidents across agency name and ensure the incident counts are integers\n",
    "df5_sum = df5.groupby('Agency_Name')['Number_of_Incidents'].sum().reset_index()\n",
    "df5_sum['Number_of_Incidents'] = df5_sum['Number_of_Incidents'].astype(int)\n",
    "\n",
    "#the agency names are really county names - let's rename this column\n",
    "df5_sum.columns = ['County_Name', 'Number_of_Incidents']\n",
    "\n",
    "#need to make the 'County_Name' lowercase so we can merge the two dfs\n",
    "df5_sum['County_Name'] = df5_sum['County_Name'].str.lower()\n",
    "df5_sum['County_Name'] = df5_sum['County_Name'].str.title()\n",
    "df5_sum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations (Maps and Charts)\n",
    "This section starts to provide a visualization of the data to allow the user the ability to better understand the data.\n",
    "\n",
    "First, lets determine the coordinates of Texas so we can center our maps correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'Texas'\n",
    "\n",
    "geolocator = Nominatim(user_agent = \"texas_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geographical coordinates of Texas are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folium Map 1 (with labels)\n",
    "Using our final database I created my first visualization using a folium map.  This map provided the locations of all 254 counties for Texas.  Initially, the county labels were all provided in lowercase - which I did not like.  This is where I decieded to reach back to the orginal database and rescrape the county names with the correct capitalization.\n",
    "\n",
    "_NOTE: the graphics might not render in github (a known error) -- so I attached the maps as screenshots in the base capstone folder on github._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_texas = folium.Map(location = [latitude, longitude], zoom_start = 6)\n",
    "\n",
    "for lat, lng, county_name, total_alcohol_sales, expenditure_amount in zip(df_final['Latitude'], \n",
    "                          df_final['Longitude'],\n",
    "                          df_final['County_Name2'], \n",
    "                          df_final['Total_Alcohol_Sales'],\n",
    "                          df_final['Expenditure_Amount']):\n",
    "    label = '{}'.format(county_name)\n",
    "    label = folium.Popup(label, parse_html = True)\n",
    "    folium.CircleMarker(\n",
    "    [lat, lng],\n",
    "    radius = 5,\n",
    "    popup = label,\n",
    "    color = 'blue',\n",
    "    fill = True,\n",
    "    fill_color = '#3186cc',\n",
    "    fill_opacity = 0.7,\n",
    "    parse_html = False).add_to(map_texas)\n",
    "\n",
    "map_texas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the map looks like polka-dots imposed over a map of Texas.  This really does not provide enough context to start an analysis.  Lets look at other options, starting with geojson data.\n",
    "\n",
    "### Geojson data map\n",
    "Using geojson data allowed me to create a map with county boundries superimposed on a map of Texas.  For my first couple of tries (multiple hours of work) the county boundries resulted in grey shading.  What I determined was the geojson data had the 'county' feature with different capitalization.  Going back to my Texas crime data - I had to complete additional data analysis to create a useful database (with correct county tags).\n",
    "\n",
    "Another issue I determined was that the Chrome web browser does not support a folium map with a large number of features.  I had to revert to FireFox (or Safari) for the map to display...this was another multiple hour dilemma.\n",
    "\n",
    "The map provided below, shows the crime data for the State of Texas in 2018.\n",
    "\n",
    "_NOTE: the graphics might not render in github (a known error) -- so I attached the maps as screenshots in the base capstone folder on github._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_geo = r'tx_counties.geojson'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "texas_map2 = folium.Map(location = [latitude, longitude], zoom_start = 6)\n",
    "\n",
    "threshold_scale = np.linspace(start = df5_sum['Number_of_Incidents'].min(),\n",
    "                              stop = df5_sum['Number_of_Incidents'].max(),\n",
    "                              num = 10)\n",
    "threshold_scale = threshold_scale.tolist()\n",
    "threshold_scale[-1] = threshold_scale[-1] + 1\n",
    "\n",
    "texas_map2.choropleth(geo_data = texas_geo,\n",
    "                      data = df5_sum,\n",
    "                      columns=['County_Name', 'Number_of_Incidents'],\n",
    "                      key_on='feature.properties.COUNTY',\n",
    "                      threshold_scale = threshold_scale,\n",
    "                      fill_color='YlOrRd',\n",
    "                      fill_opacity = 0.7,\n",
    "                      line_opacity = 1,\n",
    "                      legend_name = 'Crime in Texas'\n",
    "                     )\n",
    "\n",
    "texas_map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Better Map Data__\n",
    "This map is much better, but we still do not know which counties have the highest crime.  Maybe if we merge the two maps we can provide a better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lat, lng, county_name, total_alcohol_sales, expenditure_amount in zip(df_final['Latitude'], \n",
    "                          df_final['Longitude'],\n",
    "                          df_final['County_Name2'], \n",
    "                          df_final['Total_Alcohol_Sales'],\n",
    "                          df_final['Expenditure_Amount']):\n",
    "    label = '{}'.format(county_name)\n",
    "    label = folium.Popup(label, parse_html = True)\n",
    "    folium.CircleMarker(\n",
    "    [lat, lng],\n",
    "    radius = 5,\n",
    "    popup = label,\n",
    "    color = 'blue',\n",
    "    fill = True,\n",
    "    fill_color = '#3186cc',\n",
    "    fill_opacity = 0.7,\n",
    "    parse_html = False).add_to(texas_map2)\n",
    "\n",
    "texas_map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are five counties with the highest crime rates for 2018.\n",
    "\n",
    "1. Harris County (near Houston)\n",
    "2. Dallas County\n",
    "3. Bexar County (near San Antonio)\n",
    "4. Tarrant County (near Dallas)\n",
    "5. Travis County (near Austin)\n",
    "\n",
    "There are also a number of counties with no reported criminal incidents.  I will have to look into these counties to ensure there are no issues with the reporting.  These counties could be sparsely populated with little to no crime - but I will have to check past reporting to make sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folium Map 2 (Mixed Beverage Sales)\n",
    "Is the crime data linked to the total mixed beverage sales?  In other words does increased alcohol sales increase the crime rate in college towns?  I will create another choropleth map to provide a good visualization.  \n",
    "\n",
    "__First Issue__\n",
    "There is a large variance in the amount of alcohol sales.  The choropleth map does not like this variance when determining the correct 'bins' to place counties in.  First, lets do a quick scatter plot to double-check the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'll only use the data we need for this plot\n",
    "df_plot = df_final[['County_Number', 'Total_Alcohol_Sales']]\n",
    "df_plot['County_Number'] = df_plot['County_Number'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.plot(kind = 'scatter',\n",
    "             x = 'County_Number',\n",
    "             y = 'Total_Alcohol_Sales',\n",
    "             figsize = (15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see there are a majority (>50%) of sales between 0 and 10 million dollars.  However, there are still approximately 15% of sales much higher than the 10 million dollar threshold.  Therefore, we should manually set the threshold values for the choropleth map to control with the high variance of the data.\n",
    "\n",
    "I start with subsetting the 'total alcohol sales' into 9 equal bins.  Then create a choropleth on these new bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(start = 0, stop = df_final['Total_Alcohol_Sales'].max(), num = 9)\n",
    "x = x.tolist()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Second Issue__\n",
    "\n",
    "Well, we have a problem!! The county names in the final_df does not match the geojson data and our county names in our df are different...so lets merge dataframes on the first five letters of the county name.  This should provide a working df for the choropleth map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column with the first 5 letters of the county name\n",
    "df_final['new_col'] = df_final['County_Name2'].astype(str).str[0:4]\n",
    "df5_sum['new_col'] = df5_sum['County_Name'].astype(str).str[0:4]\n",
    "\n",
    "#merge the two df\n",
    "merged_left = pd.merge(left = df_final,\n",
    "                       right = df5_sum, \n",
    "                       how = 'left', \n",
    "                       left_on = 'new_col', \n",
    "                       right_on = 'new_col')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to the choropleth graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_map3 = folium.Map(location = [latitude, longitude], zoom_start = 6)\n",
    "\n",
    "texas_map3.choropleth(geo_data = texas_geo,\n",
    "                      data = merged_left,\n",
    "                      columns=['County_Name_y', 'Total_Alcohol_Sales'],\n",
    "                      key_on='feature.properties.COUNTY',\n",
    "                      threshold_scale = x,\n",
    "                      bins=x, \n",
    "                      fill_color='Blues',\n",
    "                      fill_opacity = 0.7,\n",
    "                      line_opacity = 0.2,\n",
    "                      legend_name = 'Alcohol Sales in Texas'\n",
    "                     )\n",
    "\n",
    "texas_map3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This map does not provide much data to work with.  Lets modify the bins and see what this does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (merged_left['Total_Alcohol_Sales'] >= 0) & (merged_left['Total_Alcohol_Sales'] <= x[1]),\n",
    "    (merged_left['Total_Alcohol_Sales'] > x[1]) & (merged_left['Total_Alcohol_Sales'] <= x[2]),\n",
    "    (merged_left['Total_Alcohol_Sales'] > x[2]) & (merged_left['Total_Alcohol_Sales'] <= x[3]),\n",
    "    (merged_left['Total_Alcohol_Sales'] > x[3]) & (merged_left['Total_Alcohol_Sales'] <= x[4]),\n",
    "    (merged_left['Total_Alcohol_Sales'] > x[4]) & (merged_left['Total_Alcohol_Sales'] <= x[5]),\n",
    "    (merged_left['Total_Alcohol_Sales'] > x[5]) & (merged_left['Total_Alcohol_Sales'] <= x[6]),\n",
    "    (merged_left['Total_Alcohol_Sales'] > x[6]) & (merged_left['Total_Alcohol_Sales'] <= x[7]),\n",
    "    (merged_left['Total_Alcohol_Sales'] > x[7]) & (merged_left['Total_Alcohol_Sales'] <= x[8])\n",
    "]\n",
    "choices = ['1', '2', '3', '4', '5', '6', '7', '8']\n",
    "merged_left['map_data'] = np.select(conditions, choices, default = '0')\n",
    "merged_left['map_data'] = merged_left['map_data'].astype(int)\n",
    "merged_left.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_map4 = folium.Map(location = [latitude, longitude], zoom_start = 6)\n",
    "\n",
    "threshold_scale = np.linspace(start = merged_left['map_data'].min(),\n",
    "                              stop = merged_left['map_data'].max(),\n",
    "                              num = 9)\n",
    "threshold_scale = threshold_scale.astype(int).tolist()\n",
    "threshold_scale[-1] = threshold_scale[-1] + 1\n",
    "\n",
    "texas_map4.choropleth(geo_data = texas_geo,\n",
    "                      data = merged_left,\n",
    "                      columns=['County_Name_y', 'map_data'],\n",
    "                      key_on='feature.properties.COUNTY',\n",
    "                      threshold_scale = threshold_scale,\n",
    "                      fill_color='Blues',\n",
    "                      fill_opacity = 0.7,\n",
    "                      line_opacity = 0.2,\n",
    "                      legend_name = 'Alcohol Sales in Texas'\n",
    "                     )\n",
    "\n",
    "texas_map4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lat, lng, county_name, total_alcohol_sales, expenditure_amount in zip(df_final['Latitude'], \n",
    "                          df_final['Longitude'],\n",
    "                          df_final['County_Name2'], \n",
    "                          df_final['Total_Alcohol_Sales'],\n",
    "                          df_final['Expenditure_Amount']):\n",
    "    label = '{}'.format(county_name)\n",
    "    label = folium.Popup(label, parse_html = True)\n",
    "    folium.CircleMarker(\n",
    "    [lat, lng],\n",
    "    radius = 5,\n",
    "    popup = label,\n",
    "    color = 'blue',\n",
    "    fill = True,\n",
    "    fill_color = '#3186cc',\n",
    "    fill_opacity = 0.7,\n",
    "    parse_html = False).add_to(texas_map4)\n",
    "\n",
    "texas_map4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the counties with the highest alcohol sales include:\n",
    "\n",
    "1. Hardin County (near Beaumont)\n",
    "2. Dallas County\n",
    "3. Travis County (near Austin)\n",
    "4. Tarrant County (near Dallas)\n",
    "5. Bexar County (near San Antonio)\n",
    "\n",
    "There seems to be some correlation with the alcohol sales and the crime rate.  Four of the top five crime areas are also in the top five for alcohol sales.  However, Harris county the top crime area does not rank in the top alcohol sales.  Harris county is actually numer 83 on the alcohol sales list, maybe something else is causing the high crime rate in this county."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folium Map 3 (Texas State Expenditures)\n",
    "\n",
    "The last dataset I looked at was the Texas state expenditures.  This dataset shows the tax dollars the Texas Government invested in counties.  One could assume the Texas state Government would invest in counties they feel are the most important to the state of Texas.  Therefore, I expect the higher amount of investment equates to a 'better' county.\n",
    "\n",
    "In this section we create the last folium map to visualize the total expenditures by county."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First Issue.__ \n",
    "This seems to be a reoccurring issue - the variance in the values are causing issues when numpy creates the threshold bins.  Lets see how different the data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = merged_left[['County_Number', 'Expenditure_Amount']]\n",
    "df_plot['County_Number'] = df_plot['County_Number'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.plot(kind = 'scatter',\n",
    "             x = 'County_Number',\n",
    "             y = 'Expenditure_Amount',\n",
    "             figsize = (15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plot we can see the majority of the data falls within 0 to 2.5 million dollars.  However, the largest expenditure amount is much higher - almost 10 times higher - therefore I will create better bins for the choropleth map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(start = 0, stop = df_final['Expenditure_Amount'].max(), num = 9)\n",
    "x = x.tolist()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions2 = [\n",
    "    (merged_left['Expenditure_Amount'] >= 0) & (merged_left['Expenditure_Amount'] <= x[1]),\n",
    "    (merged_left['Expenditure_Amount'] > x[1]) & (merged_left['Expenditure_Amount'] <= x[2]),\n",
    "    (merged_left['Expenditure_Amount'] > x[2]) & (merged_left['Expenditure_Amount'] <= x[3]),\n",
    "    (merged_left['Expenditure_Amount'] > x[3]) & (merged_left['Expenditure_Amount'] <= x[4]),\n",
    "    (merged_left['Expenditure_Amount'] > x[4]) & (merged_left['Expenditure_Amount'] <= x[5]),\n",
    "    (merged_left['Expenditure_Amount'] > x[5]) & (merged_left['Expenditure_Amount'] <= x[6]),\n",
    "    (merged_left['Expenditure_Amount'] > x[6]) & (merged_left['Expenditure_Amount'] <= x[7]),\n",
    "    (merged_left['Expenditure_Amount'] > x[7]) & (merged_left['Expenditure_Amount'] <= x[8]),\n",
    "]\n",
    "choices = ['1', '2', '3', '4', '5', '6', '7', '8']\n",
    "merged_left['map_data2'] = np.select(conditions2, choices, default = '0')\n",
    "merged_left['map_data2'] = merged_left['map_data2'].astype(int)\n",
    "merged_left.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_map5 = folium.Map(location = [latitude, longitude], zoom_start = 6)\n",
    "\n",
    "threshold_scale = np.linspace(start = merged_left['map_data2'].min().astype(int),\n",
    "                              stop = merged_left['map_data2'].max().astype(int),\n",
    "                              num = 9)\n",
    "threshold_scale = threshold_scale.astype(int).tolist()\n",
    "threshold_scale[-1] = threshold_scale[-1] + 1\n",
    "\n",
    "texas_map5.choropleth(geo_data = texas_geo,\n",
    "                      data = merged_left,\n",
    "                      columns=['County_Name_y', 'map_data2'],\n",
    "                      key_on='feature.properties.COUNTY',\n",
    "                      threshold_scale = threshold_scale,\n",
    "                      fill_color='Greens',\n",
    "                      fill_opacity = 0.7,\n",
    "                      line_opacity = 0.2,\n",
    "                      legend_name = 'Expenditure Amount by County in Texas'\n",
    "                     )\n",
    "\n",
    "\n",
    "\n",
    "texas_map5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_map5 = folium.Map(location = [latitude, longitude], zoom_start = 6)\n",
    "\n",
    "threshold_scale = np.linspace(start = merged_left['map_data2'].min().astype(int),\n",
    "                              stop = merged_left['map_data2'].max().astype(int),\n",
    "                              num = 9)\n",
    "threshold_scale = threshold_scale.astype(int).tolist()\n",
    "threshold_scale[-1] = threshold_scale[-1] + 1\n",
    "\n",
    "texas_map5.choropleth(geo_data = texas_geo,\n",
    "                      data = merged_left,\n",
    "                      columns=['County_Name_y', 'map_data2'],\n",
    "                      key_on='feature.properties.COUNTY',\n",
    "                      threshold_scale = threshold_scale,\n",
    "                      fill_color='Greens',\n",
    "                      fill_opacity = 0.7,\n",
    "                      line_opacity = 0.2,\n",
    "                      legend_name = 'Expenditure Amount by County in Texas'\n",
    "                     )\n",
    "\n",
    "for lat, lng, county_name, total_alcohol_sales, expenditure_amount in zip(df_final['Latitude'], \n",
    "                          df_final['Longitude'],\n",
    "                          df_final['County_Name2'], \n",
    "                          df_final['Total_Alcohol_Sales'],\n",
    "                          df_final['Expenditure_Amount']):\n",
    "    label = '{}'.format(county_name)\n",
    "    label = folium.Popup(label, parse_html = True)\n",
    "    folium.CircleMarker(\n",
    "    [lat, lng],\n",
    "    radius = 5,\n",
    "    popup = label,\n",
    "    color = 'blue',\n",
    "    fill = True,\n",
    "    fill_color = '#3186cc',\n",
    "    fill_opacity = 0.7,\n",
    "    parse_html = False).add_to(texas_map5)\n",
    "    \n",
    "    texas_map5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the counties with the highest state expenditures include:\n",
    "\n",
    "1. Travis County (near Austin)\n",
    "2. Harris County\n",
    "3. Dallas County\n",
    "4. Bexar County (near San Antonio)\n",
    "5. Tarrant County (near Dallas)\n",
    "\n",
    "The top counties in Texas seem to be rather constant, with four counties falling within the top 5 in all areas.  However, let's link this analysis to the top colleges we would like to focus the analysis on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## College Analysis\n",
    "For this section, I focused the analysis on the following colleges:\n",
    "* University of Texas (UT) - Travis County\n",
    "* University of Texas at San Antonio (UTSA) - Bexar County\n",
    "* University of Houston (Houston) - Harris County\n",
    "* Texas A&M (TAMU) - Brazos County\n",
    "* Texas Tech University (TTU) - Lubbock County\n",
    "\n",
    "These five colleges are the ones I am most intersted in attending.  If a stakeholder has a different list - it is easy to change, since we have the data for every county in Texas.    \n",
    "First I will subset the final dataframe to only include the data that corresponds to the list of colleges above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_df = merged_left[merged_left.County_Name2.isin(['Bexar', \n",
    "                                                        'Brazos', \n",
    "                                                        'Harris', \n",
    "                                                        'Lubbock', \n",
    "                                                        'Travis']) & \n",
    "                         merged_left.County_Name_y.isin(['Bexar County',\n",
    "                                                         'Brazos County', \n",
    "                                                         'Harris County', \n",
    "                                                         'Lubbock County', \n",
    "                                                         'Travis County'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(left = college_df,\n",
    "                       right = df5_sum, \n",
    "                       how = 'left', \n",
    "                       left_on = 'County_Name_y', \n",
    "                       right_on = 'County_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged[['County_Name', \n",
    "                       'Number_of_Incidents_y',\n",
    "                      'Total_Alcohol_Sales',\n",
    "                      'Expenditure_Amount']]\n",
    "df_merged.columns = ['County_Name',\n",
    "                    'Crime_Rate',\n",
    "                    'Alcohol_Sales',\n",
    "                    'Expenditures']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the rankings between the five colleges we picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Crime_Rate_Rank'] = df_merged['Crime_Rate'].rank(ascending = True)\n",
    "df_merged['Alcohol_Sales_Rank'] = df_merged['Alcohol_Sales'].rank(ascending = False)\n",
    "df_merged['Expenditures_Rank'] = df_merged['Expenditures'].rank(ascending = False)\n",
    "df_merged.insert(0, 'College', ['UT', 'TAMU', 'UTSA', 'TTU', 'Houston'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the total rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranked = df_merged[['College',\n",
    "                       'Crime_Rate_Rank',\n",
    "                       'Alcohol_Sales_Rank',\n",
    "                       'Expenditures_Rank']]\n",
    "df_ranked['Total'] = df_ranked.sum(axis=1)\n",
    "df_ranked = df_ranked.sort_values(by=['Total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "It looks like the __University of Texas__ is the clear winner; with the University of Texas at San Antonio and Texas Tech University in a tie for second place.  So now that we have a college to attend, where is the best place to live?  For the next part of the analysis, I will utilize the Foursquare API data and k-means clustering to determine the best location to live within Travis County â€“ where the University of Texas is located.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foursquare API Analysis  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travis County Base Map: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'Travis County, Texas'\n",
    "\n",
    "geolocator = Nominatim(user_agent = \"texas_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geographical coordinates of Travis County are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/OrndorFH01/Coursera/IBM_Capstone/Austin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_df = pd.read_csv('Texas_latitude_longitude.csv')\n",
    "zip_df = zip_df[zip_df['City'].str.contains(\"Austin\")]\n",
    "zip_df.rename(columns = {'Zip':'Zip_Code'}, inplace = True) \n",
    "zip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_zip = pd.read_csv('Austin_County.csv')\n",
    "austin_zip2 = austin_zip.loc[austin_zip['County'] == 'Travis']\n",
    "austin_zip2['Population'] = austin_zip2['Population'].astype(int)\n",
    "austin_zip2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_merge = pd.merge(austin_zip2, zip_df, on = \"Zip_Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of zipcodes that were created for Post Office boxes with an assigned population of zero.  I will go ahead and delete these 'unique' zipcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexZip = austin_merge[austin_merge['Population'] == 0].index\n",
    "austin_merge.drop(indexZip, inplace = True)\n",
    "austin_merge['Zip_Code'] = austin_merge['Zip_Code'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_merge = austin_merge.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_merge.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_texas = folium.Map(location = [latitude, longitude], zoom_start = 10.5)\n",
    "\n",
    "for lat, lng, zip_code, pop in zip(austin_merge['Latitude'], \n",
    "                          austin_merge['Longitude'],\n",
    "                          austin_merge['Zip_Code'], \n",
    "                          austin_merge['Population']):\n",
    "    label = 'Zip Code:{}\\nPopulation:{}'.format(zip_code, pop)\n",
    "    label = folium.Popup(label, parse_html = True)\n",
    "    folium.CircleMarker(\n",
    "    [lat, lng],\n",
    "    radius = 5,\n",
    "    popup = label,\n",
    "    color = 'blue',\n",
    "    fill = True,\n",
    "    fill_color = '#3186cc',\n",
    "    fill_opacity = 0.7,\n",
    "    parse_html = False).add_to(austin_texas)\n",
    "\n",
    "austin_texas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Foursquare Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'GBTVHG14LE2WKWDFJYDTF4UTKWLDY5SAULYKHNHS5DVORM5A' # your Foursquare ID\n",
    "CLIENT_SECRET = 'KWLNACCQGFTFG01N1530FPPZCGGTES4VK4AW3ZH4QDLIQU44' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Venues in Travis County.__  \n",
    "To determine the best neighborhood to live in, I will first explore the nearby venues for each zip code in Travis County."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_latitude = austin_merge.loc[0, 'Latitude'] # neighborhood latitude value\n",
    "neighborhood_longitude = austin_merge.loc[0, 'Longitude'] # neighborhood longitude value\n",
    "\n",
    "neighborhood_name = austin_merge.loc[0, 'Zip_Code'] # neighborhood name\n",
    "\n",
    "print('Latitude and longitude values of {} are {}, {}.'.format(neighborhood_name, \n",
    "                                                               neighborhood_latitude, \n",
    "                                                               neighborhood_longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 100 # limit of number of venues returned by Foursquare API\n",
    "radius = 250 # define radius\n",
    "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "    CLIENT_ID, \n",
    "    CLIENT_SECRET, \n",
    "    VERSION, \n",
    "    neighborhood_latitude, \n",
    "    neighborhood_longitude, \n",
    "    radius, \n",
    "    LIMIT)\n",
    "url # display URL\n",
    "\n",
    "results = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that extracts the category of the venue\n",
    "def get_category_type(row):\n",
    "    try:\n",
    "        categories_list = row['categories']\n",
    "    except:\n",
    "        categories_list = row['venue.categories']\n",
    "        \n",
    "    if len(categories_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return categories_list[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues = results['response']['groups'][0]['items']\n",
    "    \n",
    "nearby_venues = json_normalize(venues) # flatten JSON\n",
    "\n",
    "# filter columns\n",
    "filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']\n",
    "nearby_venues =nearby_venues.loc[:, filtered_columns]\n",
    "\n",
    "# filter the category for each row\n",
    "nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)\n",
    "\n",
    "# clean columns\n",
    "nearby_venues.columns = [col.split(\".\")[-1] for col in nearby_venues.columns]\n",
    "\n",
    "nearby_venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} venues were returned by Foursquare.'.format(nearby_venues.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearbyVenues(names, latitudes, longitudes, radius=250, LIMIT=100):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Zip_Code', \n",
    "                  'Zip_Code Latitude', \n",
    "                  'Zip_Code Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_venues = getNearbyVenues(names = austin_merge['Zip_Code'],\n",
    "                                latitudes = austin_merge['Latitude'],\n",
    "                                longitudes = austin_merge['Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(austin_venues.shape)\n",
    "austin_venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} uniques categories.'.format(len(austin_venues['Venue Category'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "austin_onehot = pd.get_dummies(austin_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add neighborhood column back to dataframe\n",
    "austin_onehot['Zip_Code'] = austin_venues['Zip_Code'] \n",
    "\n",
    "# move neighborhood column to the first column\n",
    "fixed_columns = [austin_onehot.columns[-1]] + list(austin_onehot.columns[:-1])\n",
    "austin_onehot = austin_onehot[fixed_columns]\n",
    "\n",
    "austin_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_grouped = austin_onehot.groupby('Zip_Code').mean().reset_index()\n",
    "austin_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Zip_Code']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "Zip_Code_venues_sorted = pd.DataFrame(columns=columns)\n",
    "Zip_Code_venues_sorted['Zip_Code'] = austin_grouped['Zip_Code']\n",
    "\n",
    "for ind in np.arange(austin_grouped.shape[0]):\n",
    "    Zip_Code_venues_sorted.iloc[ind, 1:] = return_most_common_venues(austin_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "Zip_Code_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "kclusters = 5\n",
    "\n",
    "austin_grouped_clustering = austin_grouped.drop('Zip_Code', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(austin_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for the first 10 rows in the dataframe\n",
    "kmeans.labels_[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "k = range (1,5)\n",
    "for n in k:\n",
    "    kmeanModel = KMeans(n_clusters = n, random_state = 0).fit(austin_grouped_clustering)\n",
    "    distortions.append(sum(np.min(cdist(austin_grouped_clustering, \n",
    "                                        kmeanModel.cluster_centers_,\n",
    "                                        'canberra'), \n",
    "                                  axis = 1))/austin_grouped_clustering.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Optimal Cluster Number(s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clustering labels\n",
    "austin_merged = austin_merge\n",
    "Zip_Code_venues_sorted['Cluster_Labels'] = kmeans.labels_\n",
    "austin_merged = austin_merged.join(Zip_Code_venues_sorted.set_index('Zip_Code'), \n",
    "                               on='Zip_Code')\n",
    "austin_merged.head() # check the last columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations (Maps and Charts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Source Code Documentation.__  I found the code for the bar chart (below) at: https://github.com/Srcanyildiz/istanbul/blob/master/Istanbul.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_venue = austin_merged\n",
    "count_venue = count_venue.drop(['Zip_Code', 'Type', 'County', 'Population', 'City', 'State',\n",
    "                               'Latitude', 'Timezone', 'Daylight savings time flag', 'geopoint'],\n",
    "                              axis = 1)\n",
    "count_venue = count_venue.groupby(['Cluster_Labels', '1st Most Common Venue']).size().reset_index(name = 'Counts')\n",
    "\n",
    "cv_cluster = count_venue.pivot(index='Cluster_Labels', columns = '1st Most Common Venue', values = 'Counts')\n",
    "cv_cluster = cv_cluster.fillna(0).astype(int).reset_index(drop = True)\n",
    "\n",
    "frame = cv_cluster.plot(kind = 'bar', figsize = (20,8), width = 0.8)\n",
    "\n",
    "plt.legend(labels = cv_cluster.columns, fontsize = 14)\n",
    "plt.title(\"Number of Venues in Each Cluster\", fontsize = 16)\n",
    "plt.xticks(fontsize = 14, rotation = 0)\n",
    "plt.xlabel(\"Number of Venues\", fontsize = 14)\n",
    "plt.ylabel(\"Clusters\", fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = austin_merged.columns[austin_merged.isnull().any()]\n",
    "print(austin_merged[austin_merged.isnull().any(axis=1)][null_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_merged = austin_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map\n",
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(austin_merged['Latitude'], \n",
    "                                  austin_merged['Longitude'], \n",
    "                                  austin_merged['Zip_Code'], \n",
    "                                  austin_merged['Cluster_Labels']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[int(cluster-1)],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[int(cluster-1)],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_merged2 = austin_merged[['Zip_Code', '1st Most Common Venue', '2nd Most Common Venue', \n",
    "                                '3rd Most Common Venue', 'Cluster_Labels']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_merged2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results  \n",
    "\n",
    "### Part 1: County Review  \n",
    "For the first part of the project, I used public data to determine the top five Texas counties in three areas.  The results of the analysis are provided below:\n",
    "\n",
    "Counties with the highest __crime rates for 2018__\n",
    "1. Harris County (near Houston)\n",
    "2. Dallas County\n",
    "3. Bexar County (near San Antonio)\n",
    "4. Tarrant County (near Dallas)\n",
    "5. Travis County (near Austin)\n",
    "\n",
    "Counties with the highest __alcohol sales__\n",
    "1. Hardin County (near Beaumont)\n",
    "2. Dallas County\n",
    "3. Travis County (near Austin)\n",
    "4. Tarrant County (near Dallas)\n",
    "5. Bexar County (near San Antonio)\n",
    "\n",
    "Counties with the highest __expenditures__ (State Investments)\n",
    "1. Travis County (near Austin)\n",
    "2. Harris County\n",
    "3. Dallas County\n",
    "4. Bexar County (near San Antonio)\n",
    "5. Tarrant County (near Dallas)\n",
    "\n",
    "### Part 2: College Review\n",
    "Using the results of our Texas county analysis, I next ranked the five colleges I was most interested in attending.\n",
    "* University of Texas (UT) - Travis County\n",
    "* University of Texas at San Antonio (UTSA) - Bexar County\n",
    "* University of Houston (Houston) - Harris County\n",
    "* Texas A&M (TAMU) - Brazos County\n",
    "* Texas Tech University (TTU) - Lubbock County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rankings showed that the University of Texas at Austin was a clear winner, placing third on crime and first in alcohol sales and expenditures.  \n",
    "\n",
    "### Part 3: Foursquare API\n",
    "Now that we have a college picked and the county ranked -- I turned to the Foursquare API data to better understand my new college town.  Using the Foursquare API data I saw a majority of the county includes venues that cater to college aged people -- coffee shops and fast food restaurants.  \n",
    "\n",
    "The town was easily split into 5 clusters using K means clustering.  The four clusters includes one with yoga; one with coffe shops and resturants; one with transportation hubs; one with recreaction; and lastly one with a golf driving range (TopGolf!!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations  \n",
    "* There is a lot of public data provided on the internet.  However, the data provided is not clean.  A majority of the time spent on the project was doing initial data cleaning and analysis\n",
    "* Not all programs are created equally.  Understanding the limitations of programs allows for a faster completion time and less stress.  During this project I determined Chrome is not adequate for choropleth maps with large json data files\n",
    "* Visualizations were key to better understanding the data\n",
    "* Austin or Travis County was first in alcohol sales, but none of the clusters focused on bars or other drinking establishments - this seems a little strange\n",
    "* Further analysis is needed to determine the lack of data for some smaller counties\n",
    "* Texas is huge and has a very large income and expenditure rate  \n",
    "\n",
    "## Conclusion\n",
    "Data driven decisions provides another tool to decision making processes.  The vast amounts of data allows data scientists the ability to provide another viewpoint -- other than the opinion based decisions we make daily.  From this project we can see that stakeholders have many schools to choose from and using data provides the first step in focusing the options for decision makers.  This was a great learning experience and it showed me the importance of data in decision making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
